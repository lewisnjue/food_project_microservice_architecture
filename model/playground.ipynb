{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q \n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "\n",
    "model_name = \"lewisnjue/my_awesome_food_model\"\n",
    "\n",
    "# Download the model and processor\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForImageClassification.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=101, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(requests.get(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\", stream=True).raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = inputs[\"pixel_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages/transformers/models/vit/modeling_vit.py:172: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if num_channels != self.num_channels:\n",
      "/home/gitpod/.pyenv/versions/3.12.9/lib/python3.12/site-packages/transformers/models/vit/modeling_vit.py:178: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if height != self.image_size[0] or width != self.image_size[1]:\n"
     ]
    }
   ],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"food_model.onnx\",\n",
    "    input_names=[\"pixel_values\"],\n",
    "    output_names=[\"logits\"],\n",
    "    dynamic_axes={\"pixel_values\": {0: \"batch_size\"}, \"logits\": {0: \"batch_size\"}},\n",
    "    opset_version=14\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model is valid!\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "onnx_model = onnx.load(\"food_model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "print(\"ONNX model is valid!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__init__.py',\n",
       " 'main.py',\n",
       " 'requirements.txt',\n",
       " '.gitignore',\n",
       " 'env',\n",
       " 'requirements.dev.txt',\n",
       " 'playground.ipynb',\n",
       " 'food_model.onnx']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_file_size_in_mb(file_path):\n",
    "    size_bytes = os.path.getsize(file_path)\n",
    "    size_mb = size_bytes / (1024 * 1024)  # Convert bytes to megabytes\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'food_model.onnx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size_mb = get_file_size_in_mb(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327.8763589859009"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "# Preprocessing parameters from your config\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "\n",
    "RESCALE_FACTOR = 0.00392156862745098  # Equivalent to dividing by 255\n",
    "IMAGE_MEAN = [0.5, 0.5, 0.5]\n",
    "IMAGE_STD = [0.5, 0.5, 0.5]\n",
    "\n",
    "def preprocess_image(image_bytes: bytes) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preprocess an image for inference with ONNX model.\n",
    "\n",
    "    Args:\n",
    "        image_bytes (bytes): Raw image bytes\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed image tensor in NCHW format\n",
    "    \"\"\"\n",
    "    # Load the image from bytes and convert to RGB\n",
    "    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "    # Resize to 224x224\n",
    "    image = image.resize((IMAGE_WIDTH, IMAGE_HEIGHT), resample=Image.BILINEAR)\n",
    "\n",
    "    # Convert image to NumPy array (HWC format)\n",
    "    img_array = np.array(image).astype(np.float32)\n",
    "\n",
    "    # Rescale pixel values by rescale factor\n",
    "    img_array *= RESCALE_FACTOR  # Same as dividing by 255\n",
    "\n",
    "    # Normalize: subtract mean and divide by std\n",
    "    img_array = (img_array - IMAGE_MEAN) / IMAGE_STD\n",
    "\n",
    "    # Transpose to CHW format (channels, height, width)\n",
    "    img_array = np.transpose(img_array, (2, 0, 1))\n",
    "\n",
    "    # Add batch dimension (1, channels, height, width)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=101, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = config.label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apple_pie': '0',\n",
       " 'baby_back_ribs': '1',\n",
       " 'baklava': '2',\n",
       " 'beef_carpaccio': '3',\n",
       " 'beef_tartare': '4',\n",
       " 'beet_salad': '5',\n",
       " 'beignets': '6',\n",
       " 'bibimbap': '7',\n",
       " 'bread_pudding': '8',\n",
       " 'breakfast_burrito': '9',\n",
       " 'bruschetta': '10',\n",
       " 'caesar_salad': '11',\n",
       " 'cannoli': '12',\n",
       " 'caprese_salad': '13',\n",
       " 'carrot_cake': '14',\n",
       " 'ceviche': '15',\n",
       " 'cheese_plate': '17',\n",
       " 'cheesecake': '16',\n",
       " 'chicken_curry': '18',\n",
       " 'chicken_quesadilla': '19',\n",
       " 'chicken_wings': '20',\n",
       " 'chocolate_cake': '21',\n",
       " 'chocolate_mousse': '22',\n",
       " 'churros': '23',\n",
       " 'clam_chowder': '24',\n",
       " 'club_sandwich': '25',\n",
       " 'crab_cakes': '26',\n",
       " 'creme_brulee': '27',\n",
       " 'croque_madame': '28',\n",
       " 'cup_cakes': '29',\n",
       " 'deviled_eggs': '30',\n",
       " 'donuts': '31',\n",
       " 'dumplings': '32',\n",
       " 'edamame': '33',\n",
       " 'eggs_benedict': '34',\n",
       " 'escargots': '35',\n",
       " 'falafel': '36',\n",
       " 'filet_mignon': '37',\n",
       " 'fish_and_chips': '38',\n",
       " 'foie_gras': '39',\n",
       " 'french_fries': '40',\n",
       " 'french_onion_soup': '41',\n",
       " 'french_toast': '42',\n",
       " 'fried_calamari': '43',\n",
       " 'fried_rice': '44',\n",
       " 'frozen_yogurt': '45',\n",
       " 'garlic_bread': '46',\n",
       " 'gnocchi': '47',\n",
       " 'greek_salad': '48',\n",
       " 'grilled_cheese_sandwich': '49',\n",
       " 'grilled_salmon': '50',\n",
       " 'guacamole': '51',\n",
       " 'gyoza': '52',\n",
       " 'hamburger': '53',\n",
       " 'hot_and_sour_soup': '54',\n",
       " 'hot_dog': '55',\n",
       " 'huevos_rancheros': '56',\n",
       " 'hummus': '57',\n",
       " 'ice_cream': '58',\n",
       " 'lasagna': '59',\n",
       " 'lobster_bisque': '60',\n",
       " 'lobster_roll_sandwich': '61',\n",
       " 'macaroni_and_cheese': '62',\n",
       " 'macarons': '63',\n",
       " 'miso_soup': '64',\n",
       " 'mussels': '65',\n",
       " 'nachos': '66',\n",
       " 'omelette': '67',\n",
       " 'onion_rings': '68',\n",
       " 'oysters': '69',\n",
       " 'pad_thai': '70',\n",
       " 'paella': '71',\n",
       " 'pancakes': '72',\n",
       " 'panna_cotta': '73',\n",
       " 'peking_duck': '74',\n",
       " 'pho': '75',\n",
       " 'pizza': '76',\n",
       " 'pork_chop': '77',\n",
       " 'poutine': '78',\n",
       " 'prime_rib': '79',\n",
       " 'pulled_pork_sandwich': '80',\n",
       " 'ramen': '81',\n",
       " 'ravioli': '82',\n",
       " 'red_velvet_cake': '83',\n",
       " 'risotto': '84',\n",
       " 'samosa': '85',\n",
       " 'sashimi': '86',\n",
       " 'scallops': '87',\n",
       " 'seaweed_salad': '88',\n",
       " 'shrimp_and_grits': '89',\n",
       " 'spaghetti_bolognese': '90',\n",
       " 'spaghetti_carbonara': '91',\n",
       " 'spring_rolls': '92',\n",
       " 'steak': '93',\n",
       " 'strawberry_shortcake': '94',\n",
       " 'sushi': '95',\n",
       " 'tacos': '96',\n",
       " 'takoyaki': '97',\n",
       " 'tiramisu': '98',\n",
       " 'tuna_tartare': '99',\n",
       " 'waffles': '100'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('label.pkl', 'wb') as file:  # 'wb' = write binary\n",
    "    pickle.dump(labels, file)\n",
    "\n",
    "print(\"Object saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded successfully!\n",
      "{'apple_pie': '0', 'baby_back_ribs': '1', 'baklava': '2', 'beef_carpaccio': '3', 'beef_tartare': '4', 'beet_salad': '5', 'beignets': '6', 'bibimbap': '7', 'bread_pudding': '8', 'breakfast_burrito': '9', 'bruschetta': '10', 'caesar_salad': '11', 'cannoli': '12', 'caprese_salad': '13', 'carrot_cake': '14', 'ceviche': '15', 'cheese_plate': '17', 'cheesecake': '16', 'chicken_curry': '18', 'chicken_quesadilla': '19', 'chicken_wings': '20', 'chocolate_cake': '21', 'chocolate_mousse': '22', 'churros': '23', 'clam_chowder': '24', 'club_sandwich': '25', 'crab_cakes': '26', 'creme_brulee': '27', 'croque_madame': '28', 'cup_cakes': '29', 'deviled_eggs': '30', 'donuts': '31', 'dumplings': '32', 'edamame': '33', 'eggs_benedict': '34', 'escargots': '35', 'falafel': '36', 'filet_mignon': '37', 'fish_and_chips': '38', 'foie_gras': '39', 'french_fries': '40', 'french_onion_soup': '41', 'french_toast': '42', 'fried_calamari': '43', 'fried_rice': '44', 'frozen_yogurt': '45', 'garlic_bread': '46', 'gnocchi': '47', 'greek_salad': '48', 'grilled_cheese_sandwich': '49', 'grilled_salmon': '50', 'guacamole': '51', 'gyoza': '52', 'hamburger': '53', 'hot_and_sour_soup': '54', 'hot_dog': '55', 'huevos_rancheros': '56', 'hummus': '57', 'ice_cream': '58', 'lasagna': '59', 'lobster_bisque': '60', 'lobster_roll_sandwich': '61', 'macaroni_and_cheese': '62', 'macarons': '63', 'miso_soup': '64', 'mussels': '65', 'nachos': '66', 'omelette': '67', 'onion_rings': '68', 'oysters': '69', 'pad_thai': '70', 'paella': '71', 'pancakes': '72', 'panna_cotta': '73', 'peking_duck': '74', 'pho': '75', 'pizza': '76', 'pork_chop': '77', 'poutine': '78', 'prime_rib': '79', 'pulled_pork_sandwich': '80', 'ramen': '81', 'ravioli': '82', 'red_velvet_cake': '83', 'risotto': '84', 'samosa': '85', 'sashimi': '86', 'scallops': '87', 'seaweed_salad': '88', 'shrimp_and_grits': '89', 'spaghetti_bolognese': '90', 'spaghetti_carbonara': '91', 'spring_rolls': '92', 'steak': '93', 'strawberry_shortcake': '94', 'sushi': '95', 'tacos': '96', 'takoyaki': '97', 'tiramisu': '98', 'tuna_tartare': '99', 'waffles': '100'}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('label.pkl', 'rb') as file:\n",
    "    loaded_labels = pickle.load(file)\n",
    "print(\"Object loaded successfully!\")\n",
    "print(loaded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
